\documentclass{article}

\title{Assignment 11.b for \textbf{STATGR6103}\\
\large submitted to Professor Andrew Gelman}
\date{23 November 2016}
\author{Advait Rajagopal}

\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{multirow,array}
\usepackage{booktabs}
\usepackage{float}
%
\usepackage[margin=1.2in,bottom=1in,top=1in]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{blindtext}

\lhead{Assignment 11.b}
\chead{}
\rhead{Advait Rajagopal}
%\lfoot{}
%\cfoot{Footer}
%\rfoot{\thepage}
\renewcommand{\headrulewidth}{1pt}
%\renewcommand{\footrulewidth}{1pt}
%
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\linespread{1.3}
\usepackage{listings}
\usepackage[most]{tcolorbox}
\usepackage{inconsolata}
\newtcblisting[auto counter]{sexylisting}[2][]{sharp corners, 
    fonttitle=\bfseries, colframe=black, listing only, 
    listing options={basicstyle=\ttfamily,language=java}, 
    title=Listing \thetcbcounter: #2, #1}


\begin{document}
  \maketitle
\section{Question 1}
Program HMC in R for the bioassay logistic regression example from Chapter 3.
\subsection{Part A}
\textbf{Code the gradients analytically and numerically and check that the two programs give the same result.}\\
It is important to write the full exposition of the model before computing the analytical and numerical gradients of the posterior distribution. 20 animals are divided  4 equally sized groups and each group is administered with a certain dosage, where the log of the dosage is denoted by $x_i$ and the number of deaths recorded in that group are denoted by $y_i$. The number of animals, in each group out of which $y_i$ die are denoted by $n_i$. If the outcomes of the five animals $(n_i = 5)$ in each group are treated as exchangeable then  $y_i$ is binomially distributed as below;
\begin{align*}
y_i|\theta_i &\sim \text{Bin}(n_i , \theta_i)
\end{align*}
Where $\theta_i$ is the probability parameter of the binomial distribution that denotes the probability of death. We use a logistic transform of $\theta$ such that
\begin{align*}
\text{logit}(\theta_i) = \alpha + \beta x_i
\end{align*}
Given the above information, the joint likelihood function for the data becomes;
\begin{align}
p(y | \alpha, \beta, n, x) &\propto \prod_{i = 1}^{k}\text{logit}^{-1}(\alpha + \beta x_i) ^{y_i} [1 - \text{logit}^{-1}(\alpha + \beta x_i) ]^{n_i - y_i} 
\end{align}
I assume noninformative priors on the parameters $\alpha$ and $\beta$ which can be summarized as below;
\begin{align}
p(\alpha,\beta) \propto 1
\end{align}
From equations 1 and 2 we infer that the joint posterior density of the parameters $\alpha$ and $\beta$ is given by the equation below,
\begin{align*}
p(\alpha, \beta | y, n, x) &\propto p(\alpha, \beta | n,x) p(y | \alpha, \beta, n, x)\\
&\propto p(\alpha, \beta) \prod_{i = 1}^{k}p(y_i | \alpha, \beta, n_i, x_i)\\
&\propto \prod_{i =1}^{5} \text{logit}^{-1}(\alpha + \beta x_i) ^{y_i} [1 - \text{logit}^{-1}(\alpha + \beta x_i) ]^{n_i - y_i} 
\end{align*}
The log of the posterior then becomes;
\begin{align}
\text{log} (p(\alpha, \beta | y, n, x)) &\propto \sum_{i = 1}^{4}y_i \text{log}(\text{logit}^{-1}(\alpha + \beta x_i)) + (n_i - y_i) \text{log}([1 - \text{logit}^{-1}(\alpha + \beta x_i) ])
\end{align}
The first derivatives\footnote{See Mathematical Appendix.} of equation 3 with respect to $\alpha$ and $\beta$ are given below;
\begin{align}
\frac{\partial \text{log}(p(\alpha, \beta | y, n, x))}{\partial \alpha} &= \sum_{i = 1}^{4} y_i - \frac{n_i (e^{\alpha + \beta x_i})}{1 + e ^{\alpha + \beta x_i}}\\
%
\frac{\partial \text{log}(p(\alpha, \beta | y, n, x))}{\partial \beta} &= \sum_{i = 1}^{4} x_iy_i - \frac{n_i x_i ( e^{\alpha + \beta x_i})}{1 + e ^{\alpha + \beta x_i}}
\end{align}
Equations 4 and 5 represent the analytical gradient. The numerical gradient on the other hand is computed by second order divided differences using the following formula;
\begin{align}
\frac{\Delta\text{log}(p(\alpha, \beta | y, n, x))}{\Delta \alpha} &= \frac{\text{log}(p(\alpha + \epsilon, \beta | y, n, x)) - \text{log}(p(\alpha - \epsilon, \beta | y, n, x))}{2\epsilon}\\
\frac{\Delta\text{log}(p(\alpha, \beta | y, n, x))}{\Delta \beta} &= \frac{\text{log}(p(\alpha , \beta+ \epsilon | y, n, x)) - \text{log}(p(\alpha , \beta - \epsilon| y, n, x))}{2\epsilon}
\end{align}
Equations 6 and 7 represent the numerical gradient as computed by the second differences formula. I calculate the gradients in equations 4,5,6 and 7 a 1000 times for values of $\alpha$ and $\beta$ generated by a uniform distribution and plot the histogram of the differences between equations 6 and 4 and 5 and 7 respectively, below. Intuitively I expect these differences to be equal to or very close to 0. 
 \begin{figure}[H]
\centering
\includegraphics[width = 15cm, height = 5cm]{histdiff.png}
\caption{We observe that the histograms are centered at zero or around the red vertical line}
\label{deltat}
\end{figure}
It is clear from Figure 1 that there is a negligible difference between the gradients of $\alpha$ and $\beta$ as computed by the analytical and numerical methods.

\subsection{Part B}
\textbf{Pick reasonable starting values for the mass matrix, step size, and number of steps.}\\
The mass matrix is called $M$, the step size is denoted by $\epsilon$ and the number of steps is $L$. According to Hamiltonian dynamics I need a momentum distribution which is denoted by  $p(\phi)$ where $\phi_j$ is the momentum variable and the parameter interest in the target density and $\phi$ get updated simultaneously. It is common to give $\phi$ a multivariate normal distribution with mean 0 and covariance equal to the diagonal mass matrix $M$. Therefore $\phi_j \sim$ N$(0, M_{jj})$ where $j = 1,..,d$ and $d$ is the number of dimensions of the parameter of the target density. As advised by Gelman et. al (2013) I allow $M$ to scale with the inverse covariance matrix of the posterior distribution. So I obtain the covariance matrix using the Nelder-Mead algorithm with the \textit{laplace} function in the \textit{LearnBayes} package. Then I compute the inverse of this covariance matrix of the posterior using the \textit{MASS} package and the \textit{ginv} function obtaining the Fisher information matrix. I set the mass matrix $M$ equal to this value obtained, ignoring the off diagonal elements for simplicity.
\begin{align*}
M &=
\begin{bmatrix} 
1.964 & 0 \\
0 & 0.086
\end{bmatrix}
\end{align*}
Again as recommended by Gelman et. al(2013) I set $\epsilon$ = 0.1 and $L = 10$. I use starting values from the normal distribution with mean 0 and standard deviation 15. I also use 2000 iterations.
The posterior estimates of the parameters are summarized in Table 1.
\begin{table} [H]
\caption {Posterior Distributions of Parameters}
\vspace{2mm}
\def\arraystretch{1.5}
\centering \begin{tabular}{c c c c c c c c c c c} 
\hline\hline 
\vspace{1mm}
 & mean & se.mean  &  sd   & 2.5\%  &  25\% &   50\% &   75\% & 97.5\% & n.eff & Rhat \\  [0.5ex] \hline
$\alpha$   &   	1.4  &   0.1 &1.2 &-0.5 &0.6 & 1.3 & 2.0 &  4.0  & 367 &   1\\
$\beta$    &   	11.9  &   0.3  &6.2 & 3.4 &7.4 &10.9& 15.3 & 27.2 &  315 &   1
 \\
\hline 
\end{tabular}
\end{table}
The acceptance probability for each of the 4 chains with the $(\epsilon, L, M)$ specification described in this section is given in Table 2.
\begin{table} [H]
\caption {Acceptance Rates}
\vspace{2mm}
\def\arraystretch{1}
\centering 
\begin{tabular}{c c c c} 
\hline 
Chain 1 & Chain 2 & Chain 3 & Chain 4 \\
1 & 1 & 1& 1 \\
\end{tabular}
\end{table}
The algorithm therefore performs quite well.
\subsection{Part C}
\textbf{Tune the algorithm to an approximate 65\% acceptance rate.}\\
I use the same mass vector $M$ but tweak $\epsilon = 1$ and $L = 3$  run for 50 iterations to give an approximate 65\% acceptance rate\footnote{See code attached.}. Posterior estimates from this configuration are given in Table 3 and the acceptance probability of the chains is given in Table 4.
\begin{table} [H]
\caption {Posterior Distributions of Parameters}
\vspace{2mm}
\def\arraystretch{1.5}
\centering \begin{tabular}{c c c c c c c c c c c} 
\hline\hline 
\vspace{1mm}
 & mean & se.mean  &  sd   & 2.5\%  &  25\% &   50\% &   75\% & 97.5\% & n.eff & Rhat \\  [0.5ex] \hline
$\alpha$   &   	1.3   &  0.2 &1.2 &-0.6 &0.4 & 1.2 & 1.9 &  3.6  &  26 & 1.2\\
$\beta$    &   	12.6  &   1.2 &6.4 & 2.4 &7.1 &11.9& 16.0 & 26.2 &   27 & 1.1
 \\
\hline 
\end{tabular}
\end{table}
\begin{table} [H]
\caption {Acceptance Rates}
\vspace{2mm}
\def\arraystretch{1}
\centering 
\begin{tabular}{c c c c} 
\hline 
Chain 1 & Chain 2 & Chain 3 & Chain 4 \\
0.75 & 0.57 & 0.65 & 0.67 \\
\end{tabular}
\end{table}
The average acceptance probability of all 4 chains is 66\%.

\subsection{Part D}
\textbf{Run 4 chains long enough so that each has an effective sample size of at least 100. How many iterations did you need?}\\
I use the $(\epsilon, L, M)$ used in section 1.2. I run 4 chains with 500 iterations and get an effective sample size of 114 and 103 for each parameter. The results of this are summarized in Table 5.
\begin{table} [H]
\caption {Posterior Distributions of Parameters}
\vspace{2mm}
\def\arraystretch{1.5}
\centering \begin{tabular}{c c c c c c c c c c c} 
\hline\hline 
\vspace{1mm}
 & mean & se.mean  &  sd   & 2.5\%  &  25\% &   50\% &   75\% & 97.5\% & n.eff & Rhat \\  [0.5ex] \hline
$\alpha$   &   	1.4  &   0.1 &1.1& -0.5 &0.6 & 1.3 & 2.0 &  4.2  & 114  &  1\\
$\beta$    &   	11.9  &   0.5 &5.5 & 3.5 &7.8 &10.8 &15.2&  24.5 &  103 &   1
 \\
\hline 
\end{tabular}
\end{table}

\subsection{Part E}
\textbf{Check that your inferences are consistent with those from the direct approach in Chapter 3.}\\
The direct approach used in Chapter 3 involves drawing 1000 draws of $\alpha$ and $\beta$ from a grid at which the posterior density is computed. The direct approach and the HMC are compared in Figure 2. I use the model specified in Table 5 from section 1.4.
\begin{figure}[H]
\centering
\includegraphics[width = 14cm, height = 6cm]{compare1.png}
\caption{(a) Shows the direct approach through grid approximation and (b) shows draws from the joint posterior estimated by HMC.}
\end{figure}
The two approaches seem quite similar.
I also compare the LD50 which is the probability of death that is 50\%.
We draw $\alpha$ and $\beta$ from the estimated posterior distribution and calculate LD50 which is the dose level at which probability of death is 50\%. Thus for us a 50\% survival rate corresponds to;
$$LD50 : E(\frac{y_i}{n_i}) = \text{logit}^{-1}(\alpha + \beta x_i) = 0.5$$
simplifying this yields LD50 = $-\alpha/\beta$. This calculation is done for the values drawn from the posterior distribution coming from the HMC algorithm as well as the direct approach and the corresponding histograms are shown in Figure 3.
\begin{figure}[H]
\centering
\includegraphics[width = 14cm, height = 6cm]{compare2.png}
\caption{(a) Histogram of LD50 with the direct approach and (b) Histogram of LD50 calculated from HMC.}
\end{figure}
The two approaches therefore appear to yield very similar results and thus they are consistent with inferences from Chapter 3.

\section{Mathematical Appendix}
\textbf{Derivation of the partial differentials of the posterior density :}\\
Consider equation 3 which shows the log posterior density;
\begin{align*}
\text{log} (p(\alpha, \beta | y, n, x)) &\propto \sum_{i = 1}^{4}y_i \text{log}(\text{logit}^{-1}(\alpha + \beta x_i)) + (n_i - y_i) \text{log}([1 - \text{logit}^{-1}(\alpha + \beta x_i) ])\\
&= \sum_{i = 1}^{4} y_i \text{log}\Big(\frac{e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}}\Big)
 + n_i \text{log} \Big( 1 - \frac{e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}}\Big) - y_i \Big( 1 - \frac{e^{\alpha + \beta x_i}}{1 + e^{\alpha + \beta x_i}}\Big)\\
 &= \sum_{i = 1}^{4} y_i [\alpha + \beta x_i - \text{log}(1 + e^{\alpha + \beta x_i})] - n_i \text{log}(1 + e^{\alpha + \beta x_i}) + y_i \text{log}(1 + e^{\alpha + \beta x_i})\\
 &= \sum_{i = 1}^{4} \alpha y_i + \beta x_i y_i - n_i \text{log}(1 + e^{\alpha + \beta x_i})
\end{align*}
Now I analytically compute the partial derivatives of the simplified log posterior density;
\begin{align*}
\frac{\partial \text{log} (p(\alpha, \beta | y, n, x))}{\partial \alpha} &= \sum_{i = 1}^{4} y_i - \frac{n_i (e^{\alpha + \beta x_i})}{1 + e ^{\alpha + \beta x_i}}\\
%
\frac{\partial \text{log}(p(\alpha, \beta | y, n, x))}{\partial \beta} &= \sum_{i = 1}^{4} x_iy_i - \frac{n_i x_i ( e^{\alpha + \beta x_i})}{1 + e ^{\alpha + \beta x_i}}
\end{align*}


\section{Code}
\subsection{R Code}
\begin{sexylisting}{R Code}
rm(list = ls())
setwd("/Users/Advait/Desktop/New School/Fall16/BDA/Class21")
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library("arm")
install.packages("LearnBayes")
library("LearnBayes")
install.packages("MASS")
library("MASS")
#####################
x <- c(-0.86, -0.3, -0.05, 0.73)
n <- rep(5,4)
y <- c(0,1,3,5)
##1Posterior
log_post <- function(param, x , y){
  param_prior <- dunif(param,-100,100, log = T)
  log_prior <- sum(param_prior)
  log_likelihood <- sum(dbinom(y, n, invlogit(param[1] 
  + param[2]*x),log = T))
  return(log_prior + log_likelihood)
}
##PART A
#Analytical gradient
gradient_an <- function(param, x,y){
  d_alpha <- sum(y - n*exp(param[1] + param[2]*x)
  /(1 + exp(param[1] + param[2]*x)))
  d_beta <- sum(y*x - n*x*exp(param[1] + param[2]*x)
  /(1 + exp(param[1] + param[2]*x)))
  return(c(d_alpha,d_beta))
}
#Numerical gradient
gradient_num <- function(param, x, y){
  d <- length(param)
  e <- 0.0001
  diff <- rep(NA,d)
 for(k in 1:d){
   th_hi <- param
   th_lo <- param
 \end{sexylisting}
 \begin{sexylisting}{R code contd.}
   th_hi[k] <- param[k] + e
   th_lo[k] <- param[k] - e
   diff[k] <- (log_post(th_hi,x,y) - log_post(th_lo,x,y))/(2*e)
  }
  return(diff)
}
#Histogram
checking <-matrix(NA ,ncol=2,nrow =1000)
for(i in 1:1000){
  random_unif <- runif(2, -10,10)
  checking[i,]<-gradient_num(random_unif ,x,y)
  -gradient_an(random_unif ,x,y)
}
options(scipen =999)
checking <- checking[complete.cases(checking),]
as.matrix(checking)
par(mfrow = c(1, 2),
    mar = c(3, 3, 1, 1),
    oma = c(.5, .5, .5, .5),
    mgp = c(2,1,0))
hist(checking[,1], breaks = 100, col = "gray",
     main = "Differences in alpha",
     xlab = "Differences",
     yaxt = "n",
     ylab = NA,
     cex.main = 1)
abline(v = 0, col = "red")
#
hist(checking[,2], breaks = 100, col = "gray",
     main = "Differences in beta",
     xlab = "Differences",
     yaxt = "n",
     ylab = NA,
     cex.main = 1)
abline(v = 0, col = "red")
###########################################################
##PART B
log_post_interim <- function(param){
  log_prior <- 0
  log_likelihood <- sum(dbinom(y, n, invlogit(param[1] 
  + param[2]*x),log = T))
  return(log_prior + log_likelihood)
}
 \end{sexylisting}
 \begin{sexylisting}{R code contd.}
lat <- laplace(log_post_interim, c(1,15))
M <- ginv(lat$var)
#HMC iter
hmc_iteration <- function(param,x,y, epsilon,L,M){
  M_inv <- 1/M
  d <- length(param)
  # Sample 10 points randomly from 
  a normal distribution with mean = 0 and standard deviation = sqrt(M)
  phi <- rnorm(d , 0, sqrt(M))
  param_old <- param
  log_p_old <- log_post(param,x,y) - 0.5*sum(M_inv*phi^2)
  phi <- phi + 0.5*epsilon*gradient_num(c(param), x, y)
  for (l in 1:L){
    param <- param + epsilon*M_inv*phi
    phi <- phi + (if (l==L)0.5 else 1)*epsilon
    *gradient_num(c(param),x,y)
  }
  phi <- -phi
  log_p_star <- log_post(c(param),x,y) - 0.5*sum(M_inv*phi^2)
  r <- exp(log_p_star - log_p_old)
  if(is.nan(r)) r <- 0
  p_jump <- min(r,1)
  param_new <- if(runif(1) < p_jump) param else param_old
  return (list (param = param_new, p_jump = p_jump))
}
##HMC run
hmc_run <- function (starting_values, iter, epsilon_0, L_0, M) {
# Get the number of rows and store in chains
chains <- nrow (starting_values)
# The number of parameters that you have in the starting values
d <- ncol (starting_values)
# Create space to store iterations of the parameters 
sims <- array (NA, c(iter, chains, d),
               dimnames=list (NULL, NULL, colnames (starting_values)))
warmup <- 0.5*iter
p_jump <- array (NA, c(iter, chains))
for (j in 1:chains){
  param <- starting_values[j,]
  for (t in 1:iter){
    epsilon <- runif (1, 0, 2*epsilon_0)
     \end{sexylisting}
 \begin{sexylisting}{R code contd.}
    L <- ceiling (2*L_0*runif(1))
    temp <- hmc_iteration (param,x,y, epsilon,L,M)
    p_jump[t,j] <- temp$p_jump
    sims[t,j,] <- temp$param
    param <- temp$param
  } }
monitor (sims, warmup)
cat ("Avg acceptance probs:",
     fround(colMeans(p_jump[(warmup+1):iter,]),2),"\n")
return (list (sims=sims, p_jump=p_jump))
}

##RUN it
parameter_names <- c (paste ("param[",1:2,"]",sep=""))
d <- 2
chains <- 4

#
mass_vector <- c(1.9648937, 0.08595404)

#Starts
starts <- array (NA,c(chains,d),dimnames=list(NULL,parameter_names))
for (j in 1:chains){
  starts[j,1] <- rnorm (1,0,15)
  starts[j,2] <- rnorm (1,0,15)
}
###PART C 
#65\% accuracy
M2 <- hmc_run (starting_values=starts, iter=100,
                 epsilon_0=1, L_0=3, M=mass_vector)
##PART D
#100 n_eff
M1 <- hmc_run (starting_values=starts, iter=500,
               epsilon_0=.1, L_0=10, M=mass_vector)
#PART E
grid_sim <- function(alpha, beta){
  x <- x
  y <- y
  n <- n
  grid_length = length(alpha)
  grid = matrix(NA, grid_length, grid_length) 
   \end{sexylisting}
 \begin{sexylisting}{R code contd.}
  for(i in 1:grid_length){
    for(j in 1:grid_length){ 
      grid[i,j] <- exp(log_post(c(alpha[i], beta[j]), x, y))
    }
  }
  return(grid/sum(grid)) #normalizing
}

sampling <- function(S=1000, grid, alpha, beta){
  marginal_alpha <- apply(grid,1,sum) 
  cdf_alpha <- cumsum(marginal_alpha)
  alpha_sim = rep(0,S) 
  beta_sim = rep(0,S) 
  for(s in 1:S){
    random_unif <- runif(1,0,1)
    f.alpha <- max(cdf_alpha[cdf_alpha <= random_unif])
    alpha_sim[s] <- alpha[cdf_alpha == f.alpha]
    beta_p <- length(alpha[alpha <= alpha_sim[s]]) 
    grid[beta_p, ] <- grid[beta_p,]/sum(grid[beta_p,]) 
    cdf_beta_con <- cumsum(grid[beta_p,]) 
    random_unif<-runif(1,0,1)
    f.beta <- max(cdf_beta_con[cdf_beta_con <= random_unif ])
    beta_sim[s] <- beta[cdf_beta_con == f.beta] }
  return(list(alpha_sim = alpha_sim,
              beta_sim = beta_sim)) }

alpha_dir <- seq(-2,8,length.out = 300) 
beta_dir <- seq(-10,39,length.out = 300)

grid <- grid_sim(alpha_dir, beta_dir)
draws <- sampling(S=1000, grid, alpha_dir, beta_dir) 
alpha_draws<-draws[[1]]
beta_draws<-draws[[2]]
mean(alpha_draws) 
mean(beta_draws) 
sd(alpha_draws) 
sd(beta_draws)
 \end{sexylisting}
 \begin{sexylisting}{R code contd.}
###
par(mfcol = c(1,2), mar = c(3, 3, 1, 1), 
    oma = c(.5, .5, .5, .5), 
    mgp = c(2,1,0))
#alpha grid scatter
plot(alpha_draws, beta_draws, pch = 16, cex = 0.4, 
main = "Direct approach", cex.main = 0.8, 
xlim = c(-4,10), ylim = c(0,40),
     xlab = "alpha", ylab = "beta")
plot(M1$sims[250:500,,1], M1$sims[250:500,,2], 
pch = 16, cex = 0.4, main = "HMC", cex.main = 0.8, 
xlim = c(-4,10), ylim = c(0,40),
     xlab = "alpha", ylab = "beta")
plot(density(alpha_draws))
abline(v = mean(alpha_draws))
plot(density(M1$sims[,,1]))
str(M1$sims)
###
#LD50
DA <- -(alpha_draws/beta_draws)
hist(DA, main = "LD 50 Direct",
     ylab = NA, yaxt = "n", breaks = 50,
     col = "gray")
abline(v = mean(DA), col = "red")

HMCA <- -(M1$sims[250:500,,1]/M1$sims[250:500,,2])
hist(HMCA, main = "LD 50 HMC",
     ylab = NA, yaxt = "n", breaks = 50,
     col = "gray")
abline(v = mean(HMCA), col = "red")

\end{sexylisting}






\end{document}