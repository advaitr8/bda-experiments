\documentclass{article}

\title{Assignment 10.b for \textbf{STATGR6103}\\
\large submitted to Professor Andrew Gelman}
\date{16 November 2016}
\author{Advait Rajagopal}

\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{multirow,array}
\usepackage{booktabs}
\usepackage{float}

\usepackage[a4paper,bindingoffset=0.2in,%
      left=1in,right=1in,top=1in,bottom=1in,%
          footskip=.25in]{geometry}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\linespread{1.3}
\usepackage{listings}
\usepackage[most]{tcolorbox}
\usepackage{inconsolata}
\newtcblisting[auto counter]{sexylisting}[2][]{sharp corners, 
    fonttitle=\bfseries, colframe=black, listing only, 
    listing options={basicstyle=\ttfamily,language=java}, 
    title=Listing \thetcbcounter: #2, #1}


\begin{document}
  \maketitle
\section{Question 1}
Consider the following discrete-data regression model: $y_i \sim \text{Poisson}(e^{X_i\beta}),$ i = 1,...,n, with independent Cauchy prior distributions with location 0 and scale 2.5 on the elements of $\beta$.

\subsection{Part A}
\textbf{Write a program in R to apply the Metropolis algorithm for $\beta$ given data X, y. Your program should work with any number of predictors (that is, X can be any matrix with the same number of rows as the length of y).}\\
The following information is given to us and a full exposition of the model is useful.
\begin{align}
p(\beta) &\sim \text{Cauchy}(0, 2.5)\\
p(y_i | X_i, \beta) &\sim \text{Poisson}(e^{X_i\beta})\\
p(\beta | X, y) &\propto p(\beta)p(y| X, \beta)
\end{align}
Equations 1, 2 and 3 give the prior, likelihood and posterior distributions respectively.
If the number of predictors is $K$ and the number of data points is $N$ then the dimensions of $y$, $\beta$ and $X$ are as follows;
\begin{align*}
[y] &= N \times 1\\
[X] & = N \times K\\
[\beta] &= K \times 1
\end{align*}
The full unnormalized posterior density is given by the following expression;
\begin{align*}
\displaystyle
p(\beta | X, y) \propto \prod_{j = 1}^{K} \frac{1}{2.5\pi} \frac{1}{\Big[ 1 + \frac{\beta_i}{2.5}^2 \Big]}\prod_{i=1}^{N}\frac{\exp(X_i\beta)^{y_i} \exp(-\exp(X_i \beta))}{y_i!}
\end{align*}
In order to avoid computational overflows and underflows, I use logarithms of the posterior densities. My jumping distribution is normal with standard deviation 0.05\footnote{See code attached in Section 2}.


\subsection{Part B}
\textbf{Simulate fake data from the model for a case with 50 data points and 3 predictors and run your program. Plot the posterior simulations from multiple chains and monitor convergence.}\\
I use my data generating function \footnote{See code attached in Section 2} to generate some fake data and Figure 1 shows the histogram of the posterior estimates. The coefficients $\beta$ are generated randomly as is the data $X$. I try to ensure that $\beta$ is not too large so that the exponential parameter of the Poisson distribution does not explode.
 \begin{figure}[H]
\centering
\includegraphics[width = 13cm, height = 5cm]{convergence1.png}
\caption{The histogram shows the posterior simulations from the Metropolis algorithm. The red line shows the true value of the parameter and the green line shows the mean of the posterior simulations.}
\label{deltat}
\end{figure}
Figure 2 shows the convergence of iterative simulations. 
 \begin{figure}[H]
\centering
\includegraphics[width = 15cm, height = 5cm]{convergence2.png}
\caption{The plot shows the convergence of iterative simulations of the sampling chains. The red line shows the true value and the green line shows the mean value as estimated by the Metropolis algorithm.}
\label{deltat}
\end{figure}

\subsection{Part C}
\textbf{Fit the model in Stan and check that you get the same results.}\\
I fit a model to the data in Stan and Figure 3 shows the posterior simulations as computed by Stan and my Metropolis algorithm. The two methods yield very similar results.
 \begin{figure}[H]
\centering
\includegraphics[width = 15cm, height = 8cm]{compare.png}
\caption{The Metropolis algorithm and Stan estimate the $\beta$'s very close to each other.}
\label{deltat}
\end{figure}

\section{Code}
\begin{sexylisting}{R code}
##PART A
###
metropolis.func <- function(n.pred, length.data){
  #generate data
  generate <- function(n.pred, length.data){
    beta.mat <<- matrix(data = rnorm(3,1), nrow = n.pred, ncol = 1)
    X.mat <<- matrix(data = NA, nrow = length.data ,ncol = n.pred)
    for(i in 1:length.data){
      for(j in 1:n.pred){
        X.mat[i,j] <<- (rnorm(1))
      }
    }
    y <<- rpois(length.data, exp(X.mat %*% beta.mat))
  }
 #Generate some data
generate(n.pred, length.data)
 ####Prior
prior <- function(param){
  prb <- NULL
  for(i in 1:length(param)){
    prb[i] <- dcauchy(param[i], 
                      location = 0, 
                      scale = 2.5, 
                      log = TRUE)
  }
  return(sum(prb))
}
####Likelihood
likelihood <- function(param){
  pred = exp(X.mat %*% param)
  lk = dpois(y, pred, log = T)
  return(sum(lk))
}
####Posterior
posterior <- function(param){
 return (likelihood(param) + prior(param))
}
####Proposal function
proposalfunction <- function(param){
  return(rnorm(length(param),
               mean = param, sd = rep(0.05,length(param))))
}
\end{sexylisting}
\begin{sexylisting}{R code contd.}
####Metropolis algorithm
run_metropolis_MCMC <- function(startvalue, iterations){
  chain = array(dim = c(iterations+1,length(startvalue)))
  chain[1,] = startvalue
  for (i in 1:iterations){
    proposal = proposalfunction(chain[i,])
    
    probab = exp(posterior(proposal) - posterior(chain[i,]))
    if (runif(1) < probab){
      chain[i+1,] = proposal
    }else{
      chain[i+1,] = chain[i,]
    }
  }
  return(chain)
}
metropolis.func.fit <<- (run_metropolis_MCMC(
rep(mean(c(beta.mat)), 
length(c(beta.mat))), 20000))
warmup <<- 10000
acceptance.rate <- 1 - mean(duplicated(
metropolis.func.fit[-(1:warmup),]))
return(paste("acceptance rate =", 
round(acceptance.rate, digits = 3)))
}

###PART B
metropolis.func(3,50)

#Histograms of parameter estimates
par(mfrow = c(1, 3), 
    mar = c(3, 3, 1, 1), 
    oma = c(.5, .5, .5, .5), 
    mgp = c(2,1,0))
parameters <- c(beta.mat)

hist((metropolis.func.fit[-(1:warmup),1]), 
     freq = FALSE, 
     xlab = "Beta1",
     col = "grey",
     main = NULL)
abline(v = parameters[1], 
       col = "red",
       lwd = 2)
       \end{sexylisting}
\begin{sexylisting}{R code contd.}
abline(v = mean(metropolis.func.fit[-(1:warmup),1]), 
       col = "green",
       lwd = 2)

hist((metropolis.func.fit[-(1:warmup),2]), 
     freq = FALSE,
     xlab = "Beta2",
     col = "grey",
     main = NULL)
abline(v = parameters[2], 
       col = "red",
       lwd = 2)
abline(v = mean(metropolis.func.fit[-(1:warmup),2]), 
       col = "green",
       lwd = 2)
hist((metropolis.func.fit[-(1:warmup),3]), 
     freq = FALSE,
     xlab = "Beta3",
     col = "grey",
     main = NULL,
     lwd = 2)
abline(v = parameters[3], 
       col = "red",
       lwd = 2)
abline(v = mean(metropolis.func.fit[-(1:warmup),3]), 
       col = "green",
       lwd = 2)
#Check convergence
par(mfrow = c(1, 3), 
    mar = c(3, 3, 1, 1), 
    oma = c(.5, .5, .5, .5), 
    mgp = c(2,1,0))

plot(metropolis.func.fit[-(1:warmup), 1], 
     type = "l",
     xlab = "Sampling Iteration",
     ylab = "Beta 1",
     main = NULL)
abline(h = parameters[1], 
       col = "red",
       lwd = 2)
abline(h = mean(metropolis.func.fit[-(1:warmup),1]), 
       col = "green",
       lwd = 2)
       \end{sexylisting}
\begin{sexylisting}{R code contd.}
plot(metropolis.func.fit[-(1:warmup), 2], 
     type = "l",
     xlab = "Sampling Iteration",
     ylab = "Beta 2",
     main = NULL)
abline(h = parameters[2], 
       col = "red",
       lwd = 2)
abline(h = mean(metropolis.func.fit[-(1:warmup),2]), 
       col = "green",
       lwd = 2)
plot(metropolis.func.fit[-(1:warmup), 3], 
     type = "l",
     xlab = "Sampling Iteration",
     ylab = "Beta 3",
     main = NULL)
abline(h = parameters[3], 
       col = "red",
       lwd = 2)
abline(h = mean(metropolis.func.fit[-(1:warmup),3]), 
       col = "green",
       lwd = 2)
###
##PART C
Xmat <- X.mat
y
nr <- 50
nc <- 3

stanc("10b.stan")
fit <- stan("10b.stan", 
            data = list("Xmat", "y"),
            iter = 1000,
            chains = 3)
print(fit)
extract(fit)
par(mfrow = c(2, 3), 
    mar = c(3, 3, 1, 1), 
    oma = c(.5, .5, .5, .5), 
    mgp = c(2,1,0))
    \end{sexylisting}
\begin{sexylisting}{R code contd.}
hist(extract(fit)$betas[,1], 
      freq = FALSE, 
      xlab = "Beta1",
      col = "grey",
      main = "Stan Beta 1")
abline(v = parameters[1], 
       col = "red",
       lwd = 2)
abline(v = mean(extract(fit)$betas[,1]),
       col = "green",
       lwd = 2)
hist(extract(fit)$betas[,2], 
     freq = FALSE, 
     xlab = "Beta2",
     col = "grey",
     main = "Stan Beta 2")
abline(v = parameters[2], 
       col = "red",
       lwd = 2)
abline(v = mean(extract(fit)$betas[,2]),
       col = "green",
       lwd = 2)
hist(extract(fit)$betas[,3], 
     freq = FALSE, 
     xlab = "Beta3",
     col = "grey",
     main = "Stan Beta 3")
abline(v = parameters[3], 
       col = "red",
       lwd = 2)
abline(v = mean(extract(fit)$betas[,3]),
       col = "green",
       lwd = 2)

####
hist((metropolis.func.fit[-(1:warmup),1]), 
     freq = FALSE, 
     xlab = "Beta1",
     col = "grey",
     main = "Met Beta1")
abline(v = parameters[1], 
       col = "red",
       lwd = 2)
       \end{sexylisting}
\begin{sexylisting}{R code contd.}
abline(v = mean(metropolis.func.fit[-(1:warmup),1]), 
       col = "green",
       lwd = 2)
hist((metropolis.func.fit[-(1:warmup),2]), 
     freq = FALSE,
     xlab = "Beta2",
     col = "grey",
     main = "Met Beta2")
abline(v = parameters[2], 
       col = "red",
       lwd = 2)
abline(v = mean(metropolis.func.fit[-(1:warmup),2]), 
       col = "green",
       lwd = 2)
hist((metropolis.func.fit[-(1:warmup),3]), 
     freq = FALSE,
     xlab = "Beta3",
     col = "grey",
     main = "Met Beta3",
     lwd = 2)
abline(v = parameters[3], 
       col = "red",
       lwd = 2)
abline(v = mean(metropolis.func.fit[-(1:warmup),3]), 
       col = "green",
       lwd = 2)
\end{sexylisting}
\subsection{Stan Code}
\begin{sexylisting}{Stan code.}
data{
int nr; 
int nc;
matrix[nr, nc] Xmat; 
int y[nr];
} 
parameters{
vector[nc] betas; 
}
model{
 y ~ poisson(exp(Xmat*betas));
  for(i in 1:nc){
    betas[i] ~ cauchy(0, 2.5);
  }
}
\end{sexylisting}

\end{document}
