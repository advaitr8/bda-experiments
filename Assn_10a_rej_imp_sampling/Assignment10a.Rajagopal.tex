\documentclass{article}

\title{Assignment 10.a for \textbf{STATGR6103}\\
\large submitted to Professor Andrew Gelman}
\date{13 November 2016}
\author{Advait Rajagopal}

\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{multirow,array}
\usepackage{booktabs}
\usepackage{float}

\usepackage[a4paper,bindingoffset=0.2in,%
      left=1in,right=1in,top=1in,bottom=1in,%
          footskip=.25in]{geometry}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\linespread{1.3}
\usepackage{listings}
\usepackage[most]{tcolorbox}
\usepackage{inconsolata}
\newtcblisting[auto counter]{sexylisting}[2][]{sharp corners, 
    fonttitle=\bfseries, colframe=black, listing only, 
    listing options={basicstyle=\ttfamily,language=java}, 
    title=Listing \thetcbcounter: #2, #1}


\begin{document}
  \maketitle
\section{Question 1}
Consider the model, $y_j \sim$ Binomial$(n_j,\theta_j)$, where $\theta_j$ = logit$^{-1}(\alpha + \beta x_j)$, for $j = 1,...,J$, and with independent prior distributions, $\alpha \sim t_4(0,2^2)$ and $\beta \sim t_4(0,1^2)$. Suppose J = 10, the $x_j$ values are randomly drawn from a U(0, 1) distribution, and $n_j \sim$ Poisson$^+(5)$, where Poisson$^+$ is the Poisson distribution restricted to positive values.
\subsection{Part A}
\textbf{Sample a dataset at random from the model.}\\
We are given the following information in the question and this helps to build the model so we can sample from it.
\begin{align}
n_j &\sim \text{Poisson}^+(5)\\
x_j &\sim \text{U}(0,1)\\
\alpha &\sim t_4(0,2^2)\\
\beta &\sim t_4(0,1^2)\\
\theta_j &= \text{logit}^{-1}(\alpha + \beta x_j)\\
y_j &\sim  \text{Binomial}(n_j,\theta_j)
\end{align}
I generate one $\alpha$ and one $\beta$ from their respective prior distributions and the values I get are $\alpha$ = -0.2908 and $\beta$ = -0.6904. Using these values I generate $n$ and $y$ using the distributions described above. The resultant values are;
\begin{align*}
n &= \{5,3,7,6,4,6,4,9,7,6 \}\\
y &= \{2,0,3,2,1,3,3,2,3,2\}
\end{align*}

\subsection{Part B}
\textbf{Use rejection sampling to get 1000 independent posterior draws from $(\alpha,\beta)$.}\\
It is useful to write out the posterior distribution of $(\alpha,\beta)$. The independent prior distributions of $\alpha$ and $\beta$ are given by equations 3 and 4.
\begin{align*}
p(\alpha,\beta | y, n, x) &\propto p(\alpha,\beta) p(y | n,  x,  \alpha, \beta) \\
&= p(\alpha)p(\beta) p(y | n,  x,  \alpha, \beta) \\
\end{align*}

\begin{align*}
\displaystyle
p(\alpha) & = \frac{\Gamma(\frac{5}{2})}{\Gamma(2)2\sqrt{4\pi}} \bigg(1 + \frac{1}{4}\bigg(\frac{\alpha}{2}\bigg)^2\bigg)^{-\frac{5}{2}} \\
p(\beta) & = \frac{\Gamma(\frac{5}{2})}{\Gamma(2)2\sqrt{4\pi}} \bigg(1 + \frac{\beta^2}{4}\bigg)^{-\frac{5}{2}}\\
\end{align*}
The likelihood function is binomial with the probability parameter $\theta_j$ as described in equation 5. Therefore the joint likelihood is given by;
\begin{align*}
\displaystyle
p(y | n, x, \alpha, \beta) &\propto \prod_{j=1}^{10}(\theta_j)^{y_j} (1 - \theta_j) ^{n_j - y_j}  \\ 
&=\prod_{j=1}^{10}\text{(logit}^{-1}(\alpha + \beta x_j))^{y_j}(1 - \text{logit}^{-1}(\alpha + \beta x_j))^{n_j - y_j}
\end{align*}
We can compute the log posterior density\footnote{I use the log posterior for computational convenience. For the rejection sampling I use exp(log$(p(\alpha, \beta |y, n, x))$.} by adding the logs of the prior and the likelihood.
\begin{align*}
\text{log}(p(\alpha,\beta | y, n, x)) &\propto \text{log}(p(\alpha)) + \text{log}(p(\beta)) + \text{log}(p(y | n, x, \alpha, \beta))\\
\text{log}(p(\alpha, \beta |y, n, x) &= -2.5\text{log}\bigg(1 + \frac{1}{4}\bigg(\frac{\alpha}{2}\bigg)^2\bigg) - 2.5\text{log}\bigg(1 + \frac{\beta^2}{4}\bigg) \\ &+ \sum_{j=1}^{10}  y_j \text{log}(\text{logit}^{-1}(\alpha + \beta x_j)) + (n_j - y_j)\text{log}(1 - \text{logit}^{-1}(\alpha + \beta x_j)) + C
\end{align*}
In order to perform rejection sampling we need to define a proposal distribution that is ideally proportional to the posterior and make sure that the ratio of the posterior density to the proposal is bounded by some constant $M$. The rejection sampling algorithm proceeds in two steps;
\begin{enumerate}
\item{Sample $\alpha$ and  $\beta$ at random from the distribution proportional to $g$.}
\item{With probability $\frac{p(\alpha, \beta | y, n, x)}{Mg(\alpha, \beta)}$ , accept $\alpha$ and $\beta$  as a draw from p. If the drawn $\alpha$ and $\beta$ are rejected return to step 1.}
\end{enumerate}
For this case I use the prior distribution of $(\alpha,\beta)$ as the proposal distribution and with some simplification I infer that $M$ is the binomial constant because the joint likelihood function $p(y | n, x, \alpha, \beta)$ has to be bounded by the binomial constant since the part that is a function of $\theta_j$ is actually less than or equal to 1.
Figure one shows a histogram of 1000 draws from the posterior and the red line shows the true distribution. It is clear that the rejection sampling algorithm is able to capture the marginal posterior densities fairly well.

 \begin{figure}[H]
\centering
\includegraphics[width = 13cm, height = 7cm]{hist1.png}
\caption{The histogram shows the 1000 draws of $\alpha$ and $\beta$ while the red line is the true $t$ distribution with scale parameters 4 and 1 respectively.}
\label{deltat}
\end{figure}

\subsection{Part C}
\textbf{Approximate the posterior density for $(\alpha,\beta)$ by a normal centered at the posterior mode with covariance matrix fit to the curvature at the mode.}\\
This task is accomplished by approximating the posterior density $p(\alpha,\beta | y, n, x)$ by using a normal distribution as follows;
\begin{align*}
p(\alpha,\beta | y, n, x) &\approx \mathcal{N}((\hat{\alpha}, \hat{\beta}), [I(\hat{\alpha}, \hat{\beta})]^{-1})
\end{align*}
Where $(\hat{\alpha}, \hat{\beta})$ is the posterior mode and $[I(\hat{\alpha}, \hat{\beta})]$ is the observed information corresponding to the second order derivative of the log of the posterior density evaluated at the mode and this the `curvature' at the mode.\\
In order to compute the posterior mode, I use the \textit{LearnBayes} package which uses a Nelder-Mead method to compute the mode and covariance matrix. The resulting joint mode and covariance matrix are given by;
\begin{align*}
\displaystyle
(\hat{\alpha}, \hat{\beta})  & = (0.151, 0.195) \\
I((\hat{\alpha}, \hat{\beta}))^{-1} & = \begin{bmatrix}
  				   0.0435 & -0.0192 \\
				 -0.0192 & 0.0667
  					\end{bmatrix}
\end{align*}

 \begin{figure}[H]
\centering
\includegraphics[width = 9cm, height = 5cm]{contour.png}
\caption{Contour plot displaying the normal approximation of the posterior density with the modal value shown in blue.}
\label{deltat}
\end{figure}


\subsection{Part D}
\textbf{Take 1000 draws from the two-dimensional $t_4$ distribution with that center and scale matrix and use importance sampling to estimate E$(\alpha|y)$ and E$(\beta|y)$.}\\
The output of the \textit{LearnBayes} package clearly gives a location and scale matrix for the normal approximation of a posterior density. We use this (explicitly mentioned in section 1.3) to draw 1000 samples $(\alpha^S, \beta^S)$ from a two dimensional $t$ distribution with 4 degrees of freedom and the center and scale mentioned in section 1.3. I further sample $(\alpha,\beta)$ from it with the probability of sampling it equal to the weight function given below;
\begin{align*}
w(\alpha, \beta) = \frac{p(\alpha, \beta | y,n,x)}{g(\alpha, \beta)}
\end{align*}
Then, I multiply the sampled $\alpha^S$ by the weights $w(\theta^S)$ and divide it by the
sum of all weighing values  $\sum_{s=1}^{S} w(\theta^S)$ (in this case 1000 values, since S = 1000). I repeat this 1000 times without replacement and divide the sum of $\alpha^S w(\theta^{S})$ by 1000 to get $E(\alpha | y)$. I repeat the process for $E(\beta | y)$ to obtain the following values for the expectations;
\begin{align*}
\displaystyle
E(\alpha | y,n,x) &=   0.1344\\
E(\beta | y,n,x) &=   0.1874\\
\end{align*}

\section{Code}
\subsection{R Code}
\begin{sexylisting}{R Code}
rm(list = ls())
install.packages("arm", dependencies = T)
install.packages("numDeriv", dependencies = T)
library("arm")
library(LearnBayes) 
library(mvtnorm) 
library(numDeriv) 
library(stats) 
set.seed(8)

N <- 10
rtpois <- function(N1, lambda){
        qpois(runif(N1, dpois(0, lambda), 1), lambda)
}
n <- rtpois(N, 5)

alpha <- 4*rt(1,4)
beta <- rt(1, 4)
x <- runif(N,0,1)
theta <- invlogit(alpha + beta*x)
#Part A
y <- rbinom(N, n,theta)

#Part B
M <- choose(n,y)
M <- prod(M)
ab.mat <- NULL
ab.mat <- data.frame(
  a.grid = 4*rt(10^4, 4), 
  b.grid = rt(10^4, 4)
)
str(ab.mat)
prior <- function(ab){
  exp(dt(ab[1],4, log = T) + dt(ab[2],4,log = T) + log(4))
}

\end{sexylisting}
\begin{sexylisting}{R code contd.}
lik <- function(ab){
  exp(sum(y*log(invlogit(ab[1] + ab[2]*x)) 
      + (n - y)*log(1 - invlogit(ab[1] + ab[2]*x))))
}
post <- function(ab){
  prior(ab)*lik(ab)
}
accept <- function(ab){
  ifelse(
   10^30 post(ab)/(prior(ab)) >= M, 
    1, 0
  )
}

attempt <- apply(cbind(ab.mat$a.grid, ab.mat$b.grid), 1, accept)
summary(attempt)
ab.mat$attempt <- attempt
thing1 <- sample(ab.mat$a.grid[ab.mat$attempt==1],1000 )
mean(thing1, na.rm = T)
thing2 <- sample(ab.mat$b.grid[ab.mat$attempt==1],1000 )
mean(thing2, na.rm = 2)

par(mfcol = c(1,2))
hist(thing1, breaks = 50, freq = F, 
     main = "Marginal posterior density of alpha",
     cex.main = 0.8, xlab = "a.grid", col = "gray")
lines(density(4*rt(10^4, 4)), col = "red")
text(x = 12, y = 0.09,
     labels = paste("mean = ", 
                    round(mean(mean(thing1, na.rm = T)),2)))
text(x = 12, y = 0.08,
     labels = paste("sd = ", 
                    round(sd(thing1, na.rm = T),2)))

hist(sample(ab.mat$b.grid[ab.mat$attempt==1],1000), 
breaks = 50, freq = F, 
     main = "Marginal posterior density of beta",
     cex.main = 0.8, xlab = "b.grid", col = "gray")
lines(density(rt(10^4, 4)), col = "red")
\end{sexylisting}
\begin{sexylisting}{R code contd.}
text(x = 6, y = 0.3,
     labels = paste("mean = ", 
                    round(mean(mean(thing2, na.rm = T)),2)))
text(x = 6, y = 0.27,
     labels = paste("sd = ", 
                    round(sd(thing2, na.rm = T),2)))
######
##Part C
log.post <- function(ab){ 
  sum(y*log(invlogit(ab[1] + ab[2]*x)) 
      + (n - y)*log(1- invlogit(ab[1] + ab[2]*x)) 
      + log(dt(ab[1], 4)) 
      + log(4) 
      + log(dt(ab[2], 4)))
}

la_t <- laplace(log.post, c(0, -1)) 
alpha.sim <- seq(-5, 5, length.out = 300) 
beta.sim <- seq(-5, 5, length.out = 300) 
la <- length(alpha.sim) 
lb <- length(beta.sim)
mode1 <- c(la_t$mode[1], la_t$mode[2]) 
sigma <- la_t$var
approx <- matrix(0, la, lb)

for(i in 1:la){ 
  for (j in 1:lb){
    approx[i,j] <- dmvnorm(c(alpha.sim[i],beta.sim[j]), mode1, sigma) 
  }
}

#contour
par(mfrow = c(1, 1), 
    mar = c(3, 3, 1, 1), 
    oma = c(.5, .5, .5, .5), 
    mgp=c(2,1,0))
contour(alpha.sim, beta.sim, approx, nlevels = 5, 
        xlim=c(-0.4, 0.6),
        ylim=c(-0.5, .7),
        xlab="Alpha", 
        ylab="Beta",
        main=NULL)
        \end{sexylisting}
\begin{sexylisting}{R code contd.}
points(la_t$mode[1],
       la_t$mode[2],
       pch=20,
       cex=1,
       col="blue") 
legend(0.44, 0.55,
       legend=c("mode"),
       col=c("blue"),
       cex=1,
       pch=c(16),
       bty="n")
##
#Part D
#Sample from the multi-t
draws <- rmvt(1000, 
              delta = mode1, 
              sigma = sigma,
              df = 4) 
alphas <- draws[,1]
betas <- draws[,2]

#Importance sampling algorithm
nsims <- 1000
post.dens <- NULL
for(i in 1:nsims){
  post.dens[i] <- exp(log.post(c(alphas[i], betas[i])))
}
g <- NULL
for(i in 1:nsims){
  g[i] <- dmvt(c(alphas[i], betas[i]),
               delta = mode1,
               sigma = sigma,
               df = 4,
               log = FALSE) 
}
e_alpha <- sum(alphas*(post.dens/sum(post.dens))/
                 (g/sum(g)))/nsims
e_alpha
e_beta <- sum(betas*(post.dens/sum(post.dens))/
                (g/sum(g)))/nsims
e_beta


\end{sexylisting}














\end{document}